1、git clone https://github.com/THUDM/GLM-4.git
2、pip install -r requirements.txt
   下载模型需要装git-lfs    #sudo apt install git-lfs
3、model: git clone https://huggingface.co/THUDM/glm-4-9b-chat-hf
4、modelscope也一样：git clone https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat-hf.git
5、/project/GLM-4-main/basic_demo/trans_cli_demo.py  #改model地址
6、python3 ./trans_cli_demo.py

量化4bit
glm3:
model_name_or_path="/home/egcs/models/chatglm3-6b"
model=AutoModel.from-pretrained(model_name_or_path,trust_remote_code=True).quantize(4).cuda()
glm4:
model_name_or_path="/home/egcs/models/glm4-9b-chat"
model=AutoModelForCausalLm.from_pretrained(
model_name_or_path,
low_cpu_mem_usage=True,
trust_remote_code=True,
load_in_4bit=True
).eval()


中文分词
分词方法：
词典、统计(Hidden Markov Model HMM)、规则、deeplearn
工具：
Jieba、THULAC、哈工大LTP （HMM、条件随机场Conditional Random Field CRF）、NLPIR、斯坦福分词器


JAX python库


安装transformers库
pip install transformers
pip install transformers==4.0

conda install -c huggingface transformers


helloword程序
from transformers import AutoModel
check_point="THUDM/chaatglm3-6b"
#model_path="/home/egcs/models/chatglm3-6b"
model=Automodel.from_pretrained(check_point,trust_remote_code=True)
print(model)


linux环境中 模型位于~/.cache/huggingface/目录下。
datasets #数据集
metrics #评估指标
models #模型架构
transformers #相关组件

p154页

开发主要步骤：
1、环境准备（Python,Pytorch,Transformers,CUDA,CuDNN）
2、模型加载 from_pretrained
3、数据预处理
4、推理与生成
5、服务部署


AutoConfig
AutoModel
AutoTokenizer


Transformers库的NLP流程三个核心组件：
1、Tokenizer (分词器) ：编码，装文本转为数字向量
2、Model (模型) ：计算并输出表示文本意义的概率分布，即Logits
3、PostProcessing （后处理）：利用产生的Logits，依据NLP任务的需求，将其转换为最终的、可解释的结果。
















