1、git clone https://github.com/THUDM/GLM-4.git
2、pip install -r requirements.txt
   下载模型需要装git-lfs    #sudo apt install git-lfs
3、model: git clone https://huggingface.co/THUDM/glm-4-9b-chat-hf
4、modelscope也一样：git clone https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat-hf.git
5、/project/GLM-4-main/basic_demo/trans_cli_demo.py  #改model地址
6、python3 ./trans_cli_demo.py

量化4bit
glm3:
model_name_or_path="/home/egcs/models/chatglm3-6b"
model=AutoModel.from-pretrained(model_name_or_path,trust_remote_code=True).quantize(4).cuda()
glm4:
model_name_or_path="/home/egcs/models/glm4-9b-chat"
model=AutoModelForCausalLm.from_pretrained(
model_name_or_path,
low_cpu_mem_usage=True,
trust_remote_code=True,
load_in_4bit=True
).eval()


中文分词
分词方法：
词典、统计(Hidden Markov Model HMM)、规则、deeplearn
工具：
Jieba、THULAC、哈工大LTP （HMM、条件随机场Conditional Random Field CRF）、NLPIR、斯坦福分词器


JAX python库


安装transformers库
pip install transformers
pip install transformers==4.0

conda install -c huggingface transformers


helloword程序
from transformers import AutoModel
check_point="THUDM/chaatglm3-6b"
#model_path="/home/egcs/models/chatglm3-6b"
model=Automodel.from_pretrained(check_point,trust_remote_code=True)
print(model)


linux环境中 模型位于~/.cache/huggingface/目录下。
datasets #数据集
metrics #评估指标
models #模型架构
transformers #相关组件

===============================================
p154
from transformers import AutoTokenizer,AutoModel
model_path="/home/egcs/models/chatglm3-6b"
tokenizer_path=model_path
tokenizer=AutoTokenizer.from_pretrained(tokenizer_path,trust_remote_code=True)
.quantize(4)
.cuda()

model=AutoModel.from_pretrained(model_path,trust_remote_code=True).quantize(4).cuda()
model=model.eval()
response,bistory=model.chat(tokenizer,"晚上睡不着应该怎么办",history=[])
print(response)
===============================================
水利数据微调模型
https://modelscope.cn/models/a135817958/ChatGLM3-water-lora

开发主要步骤：
1、环境准备（Python,Pytorch,Transformers,CUDA,CuDNN）
2、模型加载 from_pretrained
3、数据预处理
4、推理与生成
5、服务部署


AutoConfig
AutoModel
AutoTokenizer


Transformers库的NLP流程三个核心组件：
1、Tokenizer (分词器) ：编码，装文本转为数字向量
2、Model (模型) ：计算并输出表示文本意义的概率分布，即Logits
3、PostProcessing （后处理）：利用产生的Logits，依据NLP任务的需求，将其转换为最终的、可解释的结果。


==============================================================================================
国内开源模型
Qwen DeepSeek ChatGLM


模型加载和解读
1）编码器（Encoder）模型，如BERT ，文本分类、实体识别
2) 解码器（Decoder）模型，以GPT(1,2,3)，文本生成、机器翻译
3）序列到序列（Sequence-to-Sequence）模型，如BART，理解输入序列并生成新序列的任务。 ChatGLM也算。

模型量化
AWQ 指令微调模型需低精度量化且对精度敏感时（如企业级Agent）
GPTQ 需高性能GPU推理且追求极致压缩率时 （如云端部署）
GGUF 本地或边缘设备部署，需平衡资源与性能时（如个人开发者）

git clone https://www.modelscope.cn/OuteAI/OuteTTS-0.3-1B-GGUF.git


#distilbert-base-uncased模型
from transformers import AutoTokenizer,AutoModel
tokenizer=AutoTokenizer.from_pretrained('/home/yematech/glm/distilbert-base-uncased',trust_remote_code=True)
model=AutoModel.from_pretrained('/home/yematech/glm/distilbert-base-uncased',trust_remote_code=True).half().cuda()
print(model)


Transformers库:模型架构的实现  from_pretrained
Tokenizers:文本的预处理
Datasets:数据加载和数据处理，提供给模型训练。  load_dataset
Accelerate:整个生命周期管理


#
from datasets import load_dataset
dataset=load_dataset("/home/xuhui/models/datasets/yelp-review-full")
print(dataset)
print(dataset["train"][0])

raw_datasets=load_dataset("glue","mrpc")
raw_datasets



















